{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96c78c0-9430-4759-a756-e54d14af60fe",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åº“å¯¼å…¥å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import pydicom as dicomio \n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# è®© matplotlib å›¾è¡¨ç›´æ¥åœ¨ Jupyter ä¸­æ˜¾ç¤º\n",
    "%matplotlib inline \n",
    "\n",
    "# å¯¼å…¥æœ¬åœ°æ¨¡å—\n",
    "try:\n",
    "    from loss import TverskyLoss, MixedLoss\n",
    "    from net import UNet_2D, UNet_3D\n",
    "    from volume_patch_composer import volume_composer, patch_creator\n",
    "    from dataset import Pancreas_2D_dataset, Pancreas_3D_dataset, partitioning\n",
    "    from metrics import performance_metrics\n",
    "    from train import train_2D, train_3D\n",
    "    from inference import get_inference_performance_metrics_3D\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„æ¨¡å—æ–‡ä»¶ (å¦‚ net.py, loss.py)ã€‚\\nè¯¦ç»†ä¿¡æ¯: {e}\")\n",
    "\n",
    "print(\"åº“å¯¼å…¥å®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b943f81-bd57-46d7-8043-aba9e3326a82",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é…ç½®å·²åŠ è½½ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================\n",
    "CONFIG = {\n",
    "    'raw_ct_dir': './Pancreas-CT',              \n",
    "    'raw_label_dir': './Pancreas-CT-Label',     \n",
    "    'processed_2d_dir': './data',               \n",
    "    'processed_3d_dir': './data3D',             \n",
    "    \n",
    "    'unet_2d': False,              \n",
    "    'batch_size': 32,                \n",
    "    'num_workers': 20,               \n",
    "    'n_epochs': 50,                \n",
    "    'inference_only': False,       \n",
    "    'train_on_gpu': torch.cuda.is_available(),\n",
    "    'seed': 51\n",
    "}\n",
    "\n",
    "# ================= ğŸ› ï¸ å·¥å…·å‡½æ•° =================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def process_ct_window(ct_array, w_level=40, w_width=400):\n",
    "    min_val = w_level - w_width / 2\n",
    "    max_val = w_level + w_width / 2\n",
    "    ct_clipped = np.clip(ct_array, min_val, max_val)\n",
    "    ct_norm = (ct_clipped - min_val) / (max_val - min_val)\n",
    "    ct_norm = ct_norm * 255.0\n",
    "    return ct_norm.astype(np.uint8)\n",
    "\n",
    "def prepare_directories():\n",
    "    for p in [CONFIG['processed_2d_dir'], CONFIG['processed_3d_dir']]:\n",
    "        if not os.path.exists(p):\n",
    "            os.makedirs(p)\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "set_seed(CONFIG['seed'])\n",
    "print(\"é…ç½®å·²åŠ è½½ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22082e8a-ed95-48f5-bfa9-987222c4632d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data_robust():\n",
    "    \"\"\"é²æ£’çš„æ•°æ®é¢„å¤„ç†å‡½æ•°\"\"\"\n",
    "    print(\"--- æ£€æŸ¥æ•°æ®çŠ¶æ€ ---\")\n",
    "    check_patient = os.path.join(CONFIG['processed_2d_dir'], 'Patient0082', 'CT')\n",
    "    if os.path.exists(check_patient) and len(os.listdir(check_patient)) > 0:\n",
    "        print(\"âœ… æ£€æµ‹åˆ°æœ¬åœ°å·²æœ‰é¢„å¤„ç†æ•°æ® (./data)ï¼Œè·³è¿‡ PNG ç”Ÿæˆæ­¥éª¤ã€‚\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸ”„ æœªæ‰¾åˆ°å®Œæ•´æ•°æ®ï¼Œå¼€å§‹æ‰§è¡Œé¢„å¤„ç† (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...\")\n",
    "    prepare_directories()\n",
    "\n",
    "    # æ£€æŸ¥ pydicom ç‰ˆæœ¬å…¼å®¹æ€§\n",
    "    try:\n",
    "        if not hasattr(dicomio, 'dcmread'):\n",
    "            dicomio.dcmread = dicomio.read_file\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i in range(1, 83):\n",
    "        patient_id = '{:04d}'.format(i)\n",
    "        \n",
    "        # è·¯å¾„å‡†å¤‡\n",
    "        nifti_filename = f\"label{patient_id}.nii.gz\"\n",
    "        nifti_path = os.path.join(CONFIG['raw_label_dir'], nifti_filename)\n",
    "        ct_folder_pattern = os.path.join(CONFIG['raw_ct_dir'], f\"PANCREAS_{patient_id}\", \"**\", \"*.dcm\")\n",
    "        \n",
    "        # 1. æ£€æŸ¥æºæ–‡ä»¶\n",
    "        if not os.path.exists(nifti_path):\n",
    "            # print(f\"âš ï¸  [Patient {patient_id}] è·³è¿‡: æ‰¾ä¸åˆ°æ ‡ç­¾æ–‡ä»¶\")\n",
    "            continue\n",
    "        \n",
    "        dcm_files = glob.glob(ct_folder_pattern, recursive=True)\n",
    "        if not dcm_files:\n",
    "            # print(f\"âš ï¸  [Patient {patient_id}] è·³è¿‡: æ‰¾ä¸åˆ° DICOM æ–‡ä»¶\")\n",
    "            continue\n",
    "\n",
    "        # 2. è¯»å–å¹¶æ’åº DICOM\n",
    "        try:\n",
    "            slices = []\n",
    "            for f in dcm_files:\n",
    "                try:\n",
    "                    ds = dicomio.dcmread(f)\n",
    "\n",
    "                    # å…ˆè½¬ä¸º float é¿å…è®¡ç®—æº¢å‡º\n",
    "                    image = ds.pixel_array.astype(np.float32)\n",
    "                    \n",
    "                    # åº”ç”¨æ–œç‡å’Œæˆªè· (å¦‚æœå­˜åœ¨)\n",
    "                    if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n",
    "                        slope = float(ds.RescaleSlope)\n",
    "                        intercept = float(ds.RescaleIntercept)\n",
    "                        image = image * slope + intercept\n",
    "\n",
    "                    # slices.append(ds)\n",
    "                    slices.append((float(ds.ImagePositionPatient[2]), image))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            if not slices:\n",
    "                continue\n",
    "\n",
    "            # æŒ‰ Z è½´ä½ç½®æ’åº\n",
    "            # slices.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n",
    "            slices.sort(key=lambda x: x[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ [Patient {patient_id}] å¤„ç†å´©æºƒ: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 3. è¯»å– Mask\n",
    "        try:\n",
    "            mask_obj = nib.load(nifti_path)\n",
    "            mask_data = mask_obj.get_fdata()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ [Patient {patient_id}] NIfTI è¯»å–å¤±è´¥: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 4. å¯¹é½å±‚æ•°\n",
    "        num_dcm = len(slices)\n",
    "        num_mask = mask_data.shape[2]\n",
    "        valid_slices = min(num_dcm, num_mask)\n",
    "        \n",
    "        if valid_slices < 10:\n",
    "            continue\n",
    "            \n",
    "        # 5. ä¿å­˜ PNG\n",
    "        save_dir_ct = os.path.join(CONFIG['processed_2d_dir'], 'Patient' + patient_id, 'CT')\n",
    "        save_dir_mask = os.path.join(CONFIG['processed_2d_dir'], 'Patient' + patient_id, 'Masks')\n",
    "        os.makedirs(save_dir_ct, exist_ok=True)\n",
    "        os.makedirs(save_dir_mask, exist_ok=True)\n",
    "\n",
    "        # ... (å‰é¢çš„ä»£ç ä¿æŒä¸å˜)\n",
    "        try:\n",
    "            for s in range(valid_slices):\n",
    "                mask_slice = mask_data[:, :, s]\n",
    "                \n",
    "                # è·å–åŸå§‹ CT æ•°æ®\n",
    "                # raw_ct_slice = slices[s].pixel_array.transpose(1, 0)\n",
    "                raw_ct_slice = slices[s][1].transpose(1, 0)\n",
    "                \n",
    "                # --- ğŸ”¥ ä¿®æ”¹å¼€å§‹ ğŸ”¥ ---\n",
    "                # 1. å¯¹ CT è¿›è¡Œçª—ä½è°ƒæ•´å’Œå½’ä¸€åŒ– (å…³é”®ä¿®å¤!)\n",
    "                processed_ct_slice = process_ct_window(raw_ct_slice, w_level=40, w_width=400)\n",
    "                \n",
    "                # 2. ç¡®ä¿ Mask ä¹Ÿæ˜¯ uint8 æ ¼å¼ (0 å’Œ 255, æˆ–è€… 0 å’Œ 1)\n",
    "                # å»ºè®®å°† Mask ä¹˜ä»¥ 255 ä»¥ä¾¿è‚‰çœ¼è§‚å¯Ÿï¼Œä½†åœ¨è¯»å–æ—¶è¦é™¤å›æ¥\n",
    "                mask_slice = (mask_slice * 255).astype(np.uint8)\n",
    "                # è¿™é‡Œä¸ºäº†å…¼å®¹ä½ ç°æœ‰çš„ dataset ä»£ç (å‡è®¾å®ƒè¯»å–0/1)ï¼Œæˆ‘ä»¬ä¿æŒ 0/1 ä½†è½¬ä¸º uint8\n",
    "                # mask_slice = mask_slice.astype(np.uint8)\n",
    "                # --- ğŸ”¥ ä¿®æ”¹ç»“æŸ ğŸ”¥ ---\n",
    "\n",
    "                filename = f\"{s:04d}.png\"\n",
    "                cv2.imwrite(os.path.join(save_dir_mask, filename), mask_slice)\n",
    "                cv2.imwrite(os.path.join(save_dir_ct, filename), processed_ct_slice)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ [Patient {patient_id}] ä¿å­˜å‡ºé”™: {e}\")      \n",
    "\n",
    "    print(\"--- æ•°æ®é¢„å¤„ç†å®Œæˆ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703cd75d-3c85-4b5d-a458-851365f1377a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ£€æŸ¥æ•°æ®çŠ¶æ€ ---\n",
      "âœ… æ£€æµ‹åˆ°æœ¬åœ°å·²æœ‰é¢„å¤„ç†æ•°æ® (./data)ï¼Œè·³è¿‡ PNG ç”Ÿæˆæ­¥éª¤ã€‚\n",
      "æ„å»ºæ–‡ä»¶ç´¢å¼•...\n",
      "æœ‰æ•ˆç—…ä¾‹æ•°: 80\n",
      "æ£€æŸ¥ 3D æ•°æ®ç¼“å­˜...\n",
      "âœ… æ‰€æœ‰ 3D æ•°æ® (.pt) å·²å­˜åœ¨ï¼Œè·³è¿‡é‡é‡‡æ ·æ­¥éª¤ã€‚\n"
     ]
    }
   ],
   "source": [
    "# 1. æ‰§è¡Œé¢„å¤„ç†\n",
    "preprocess_data_robust()\n",
    "\n",
    "# 2. æ„å»ºæ•°æ®ç´¢å¼•\n",
    "print(\"æ„å»ºæ–‡ä»¶ç´¢å¼•...\")\n",
    "patient_path_list = {'CT': {}, 'Masks': {}}\n",
    "patient_image_cnt_CT = {}\n",
    "patient_image_cnt_Mask = {}\n",
    "valid_patients = []\n",
    "patient_dirs = sorted(glob.glob(os.path.join(CONFIG['processed_2d_dir'], 'Patient*')))\n",
    "\n",
    "for p_dir in patient_dirs:\n",
    "    p_key = os.path.basename(p_dir)\n",
    "    ct_files = sorted(glob.glob(os.path.join(p_dir, 'CT', '*.png')))\n",
    "    mask_files = sorted(glob.glob(os.path.join(p_dir, 'Masks', '*.png')))\n",
    "    if len(ct_files) > 0 and len(ct_files) == len(mask_files):\n",
    "        patient_path_list['CT'][p_key] = ct_files\n",
    "        patient_path_list['Masks'][p_key] = mask_files\n",
    "        patient_image_cnt_CT[p_key] = len(ct_files)\n",
    "        patient_image_cnt_Mask[p_key] = len(mask_files)\n",
    "        valid_patients.append(p_key)\n",
    "\n",
    "print(f\"æœ‰æ•ˆç—…ä¾‹æ•°: {len(valid_patients)}\")\n",
    "\n",
    "# 3. 3D æ•°æ®ç¼“å­˜\n",
    "print(\"æ£€æŸ¥ 3D æ•°æ®ç¼“å­˜...\")\n",
    "d1 = torch.linspace(-1, 1, 256)\n",
    "d2 = torch.linspace(-1, 1, 256)\n",
    "d3 = torch.linspace(-1, 1, 128)\n",
    "meshx, meshy, meshz = torch.meshgrid((d1, d2, d3), indexing='ij')\n",
    "grid = torch.stack((meshx, meshy, meshz), 3).unsqueeze(0)\n",
    "\n",
    "new_pt_count = 0\n",
    "for patient in valid_patients:\n",
    "    out_ct_path = os.path.join(CONFIG['processed_3d_dir'], patient + '_CT.pt')\n",
    "    if not os.path.exists(out_ct_path):\n",
    "        try:\n",
    "            volume_composer(patient, patient_image_cnt_CT, patient_path_list, grid)\n",
    "            new_pt_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Resizing {patient} error: {e}\")\n",
    "\n",
    "if new_pt_count == 0:\n",
    "    print(\"âœ… æ‰€æœ‰ 3D æ•°æ® (.pt) å·²å­˜åœ¨ï¼Œè·³è¿‡é‡é‡‡æ ·æ­¥éª¤ã€‚\")\n",
    "else:\n",
    "    print(f\"ğŸ”„ æ–°ç”Ÿæˆäº† {new_pt_count} ä¸ª 3D æ•°æ®æ–‡ä»¶ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21ee6e7-d70d-4cb1-8c83-1777aec83ceb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡ Dataset...\n",
      "train:  56   valid:  7   test:  17   total:  80\n",
      "åŠ è½½ Patches (è¿™æ­¥éœ€è¦ä¸€ç‚¹å†…å­˜)...\n",
      "Filtering patches... Original size: 448\n",
      "Filtered size: 448\n",
      "DataLoader å‡†å¤‡å°±ç»ªã€‚\n"
     ]
    }
   ],
   "source": [
    "# 4. è®­ç»ƒå‡†å¤‡\n",
    "print(\"å‡†å¤‡ Dataset...\")\n",
    "part = partitioning(valid_patients, split_ratio=[0.7, 0.1, 0.2])\n",
    "\n",
    "# å¢åŠ patchå¤§å°ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡ã€æ•°æ®å¢å¼ºå°ºå¯¸ã€æ¨¡å‹åˆå§‹ç‰¹å¾å›¾æ•°é‡\n",
    "# kc, kh, kw = 32, 64, 64\n",
    "# dc, dh, dw = 32, 64, 64\n",
    "kc, kh, kw = 48, 96, 96  \n",
    "dc, dh, dw = 48, 96, 96\n",
    "\n",
    "CT_patches = {}\n",
    "mask_patches = {}\n",
    "\n",
    "print(\"åŠ è½½ Patches (è¿™æ­¥éœ€è¦ä¸€ç‚¹å†…å­˜)...\")\n",
    "for p in ['train', 'valid']:\n",
    "    CT_patches[p], mask_patches[p] = patch_creator(part[p], kw, kh, kc, dw, dh, dc)\n",
    "\n",
    "dataset_train = Pancreas_3D_dataset(CT_patches['train'], mask_patches['train'], augment=True, is_train=True)\n",
    "dataset_valid = Pancreas_3D_dataset(CT_patches['valid'], mask_patches['valid'], augment=False, is_train=False)\n",
    "\n",
    "loaders = {\n",
    "    'train': torch.utils.data.DataLoader(dataset_train, batch_size=CONFIG['batch_size'], \n",
    "                                         shuffle=True, num_workers=CONFIG['num_workers'],pin_memory=True),# å¯ç”¨å†…å­˜é”å®šï¼ŒåŠ é€Ÿæ•°æ®ä¼ è¾“\n",
    "    'valid': torch.utils.data.DataLoader(dataset_valid, batch_size=CONFIG['batch_size'], \n",
    "                                         shuffle=False, num_workers=CONFIG['num_workers'],pin_memory=True)\n",
    "}\n",
    "print(\"DataLoader å‡†å¤‡å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0cf5dc-4f55-48af-9f54-355e3bef9ae6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹åŒ–æ¨¡å‹...\n",
      "ğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: results/run_20251129-2112_model.pt\n",
      "âœ… åŠ è½½æˆåŠŸï¼å°†åœ¨ç°æœ‰åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒã€‚\n",
      "æ¨¡å‹åˆå§‹åŒ–å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# 5. åˆå§‹åŒ–æ¨¡å‹ä¸ä¼˜åŒ–å™¨\n",
    "print(\"åˆå§‹åŒ–æ¨¡å‹...\")\n",
    "model = UNet_3D(1, 1, 32, 0.3)\n",
    "if CONFIG['train_on_gpu']:\n",
    "    model.cuda()        \n",
    "\n",
    "# ================= å…³é”®ä¿®æ”¹ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ =================\n",
    "# âš ï¸ æ³¨æ„ï¼šè¯·ç¡®è®¤æ–‡ä»¶åæ˜¯å¦æ­£ç¡®\n",
    "checkpoint_path = 'results/run_20251129-2112_model.pt' \n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"ğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(\"âœ… åŠ è½½æˆåŠŸï¼å°†åœ¨ç°æœ‰åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚\")     \n",
    "# ========================================================\n",
    "\n",
    "# âœ… ä½¿ç”¨æ–°çš„æ··åˆ Loss\n",
    "# alpha=0.7 å¼ºè°ƒå¬å›ï¼Œbce_weight=0.5 æä¾›æ¢¯åº¦å¹³æ»‘\n",
    "criterion = MixedLoss(alpha=0.8, beta=0.2, bce_weight=0.2)\n",
    "\n",
    "# å®šä¹‰ Loss (è¿™é‡Œç”¨äº†ä½ æ–°çš„å‚æ•°)\n",
    "# criterion = TverskyLoss(1e-6, 0.7, 0.3)\n",
    "# 1. å®šä¹‰åŸºç¡€ä¼˜åŒ–å™¨ (LR ä¼šè¢« Scheduler è¦†ç›–ï¼Œæ‰€ä»¥è¿™é‡Œåˆå§‹ LR å¯ä»¥éšæ„ï¼Œä½†å»ºè®®è®¾ä¸º max_lr çš„ 1/10 æˆ– 1/25)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)  # åœ¨ä¼˜åŒ–å™¨ä¸­å¢åŠ æƒé‡è¡°å‡\n",
    "\n",
    "# 2.å®šä¹‰ OneCycleLR\n",
    "# max_lr: æœ€å¤§å­¦ä¹ ç‡ï¼Œå¯ä»¥å°è¯• 1e-3 æˆ– 5e-4\n",
    "# steps_per_epoch: æ¯ä¸ª epoch çš„ batch æ•°é‡\n",
    "# epochs: æ€» epoch æ•°\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer, \n",
    "#     max_lr=5e-4, \n",
    "#     steps_per_epoch=len(loaders['train']), \n",
    "#     epochs=CONFIG['n_epochs']\n",
    "# )\n",
    "\n",
    "# ä½™å¼¦é€€ç«çƒ­èº«é‡å¯\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#     optimizer, \n",
    "#     T_0=20,           # åˆå§‹å‘¨æœŸé•¿åº¦\n",
    "#     T_mult=2,         # å‘¨æœŸå€å¢å› å­\n",
    "#     eta_min=1e-6\n",
    "# )\n",
    "\n",
    "#å½“æŒ‡æ ‡è¿›å…¥å¹³åŸæœŸï¼ˆä¸å†å˜åŒ–ï¼‰æ—¶ï¼Œé™ä½å­¦ä¹ ç‡ã€‚\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',      # ç›‘æ§æŒ‡æ ‡æ˜¯è¶Šå°è¶Šå¥½ (Loss)\n",
    "    factor=0.5,      # æ¯æ¬¡é™ä½ä¸€åŠ (0.1 ä¹Ÿå¯ä»¥)\n",
    "    patience=10,     # å®¹å¿ 10 ä¸ª Epoch ä¸ä¸‹é™\n",
    ")\n",
    "\n",
    "print(\"æ¨¡å‹åˆå§‹åŒ–å®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b39721d-cb08-4639-abed-a5e3ebfe9b54",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹è®­ç»ƒ: run_20251130-0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch #1 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1/51 [00:56<47:22, 56.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.5522 \tValidation Loss: 0.5601\n",
      "Specificity: 0.997593 \tSensitivity: 0.444288 \tF2_score: 0.442052 \tDSC: 0.438906\n",
      "Validation DSC increased.  Saving model ...\n",
      "=== Epoch #2 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 2/51 [01:16<28:39, 35.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 0.5393 \tValidation Loss: 0.5767\n",
      "Specificity: 0.997634 \tSensitivity: 0.424102 \tF2_score: 0.424591 \tDSC: 0.425414\n",
      "=== Epoch #3 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 3/51 [01:36<22:36, 28.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 0.5903 \tValidation Loss: 0.5538\n",
      "Specificity: 0.996664 \tSensitivity: 0.486406 \tF2_score: 0.459894 \tDSC: 0.425179\n",
      "=== Epoch #4 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4/51 [01:56<19:29, 24.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 0.5913 \tValidation Loss: 0.5802\n",
      "Specificity: 0.997278 \tSensitivity: 0.434472 \tF2_score: 0.426788 \tDSC: 0.415836\n",
      "=== Epoch #5 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 5/51 [02:16<17:46, 23.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 0.5554 \tValidation Loss: 0.5735\n",
      "Specificity: 0.997454 \tSensitivity: 0.433436 \tF2_score: 0.429375 \tDSC: 0.423556\n",
      "=== Epoch #6 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 6/51 [02:36<16:33, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 0.5654 \tValidation Loss: 0.5653\n",
      "Specificity: 0.997527 \tSensitivity: 0.441084 \tF2_score: 0.437884 \tDSC: 0.433244\n",
      "=== Epoch #7 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 7/51 [02:56<15:43, 21.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 0.5584 \tValidation Loss: 0.5495\n",
      "Specificity: 0.996924 \tSensitivity: 0.482096 \tF2_score: 0.461475 \tDSC: 0.433785\n",
      "=== Epoch #8 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 8/51 [03:16<15:01, 20.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 0.5469 \tValidation Loss: 0.5688\n",
      "Specificity: 0.996844 \tSensitivity: 0.460033 \tF2_score: 0.440954 \tDSC: 0.415132\n",
      "=== Epoch #9 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 9/51 [03:37<14:41, 21.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 0.5639 \tValidation Loss: 0.5700\n",
      "Specificity: 0.997194 \tSensitivity: 0.448650 \tF2_score: 0.437882 \tDSC: 0.422677\n",
      "=== Epoch #10 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–‰        | 10/51 [03:59<14:34, 21.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 0.5339 \tValidation Loss: 0.5642\n",
      "Specificity: 0.997364 \tSensitivity: 0.448423 \tF2_score: 0.441233 \tDSC: 0.430883\n",
      "=== Epoch #11 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 11/51 [04:20<14:08, 21.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 0.5290 \tValidation Loss: 0.5959\n",
      "Specificity: 0.997758 \tSensitivity: 0.400577 \tF2_score: 0.405444 \tDSC: 0.412972\n",
      "=== Epoch #12 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 12/51 [04:40<13:34, 20.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 0.5620 \tValidation Loss: 0.5675\n",
      "Specificity: 0.997694 \tSensitivity: 0.432811 \tF2_score: 0.434140 \tDSC: 0.436189\n",
      "=== Epoch #13 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 13/51 [05:02<13:16, 20.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 0.5925 \tValidation Loss: 0.5708\n",
      "Specificity: 0.997325 \tSensitivity: 0.441035 \tF2_score: 0.433993 \tDSC: 0.423847\n",
      "=== Epoch #14 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 14/51 [05:22<12:48, 20.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 0.5535 \tValidation Loss: 0.5676\n",
      "Specificity: 0.997219 \tSensitivity: 0.449632 \tF2_score: 0.439284 \tDSC: 0.424650\n",
      "=== Epoch #15 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 15/51 [05:42<12:16, 20.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 0.5300 \tValidation Loss: 0.5513\n",
      "Specificity: 0.997162 \tSensitivity: 0.469870 \tF2_score: 0.456047 \tDSC: 0.436803\n",
      "=== Epoch #16 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [06:02<11:50, 20.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \tTraining Loss: 0.5882 \tValidation Loss: 0.6332\n",
      "Specificity: 0.998183 \tSensitivity: 0.344057 \tF2_score: 0.359852 \tDSC: 0.386469\n",
      "=== Epoch #17 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 17/51 [06:22<11:30, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \tTraining Loss: 0.5295 \tValidation Loss: 0.5689\n",
      "Specificity: 0.997345 \tSensitivity: 0.441651 \tF2_score: 0.434963 \tDSC: 0.425303\n",
      "=== Epoch #18 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [06:42<11:08, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \tTraining Loss: 0.5752 \tValidation Loss: 0.6145\n",
      "Specificity: 0.997770 \tSensitivity: 0.376473 \tF2_score: 0.382969 \tDSC: 0.393314\n",
      "=== Epoch #19 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 19/51 [07:01<10:38, 19.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 \tTraining Loss: 0.5501 \tValidation Loss: 0.5587\n",
      "Specificity: 0.997461 \tSensitivity: 0.449716 \tF2_score: 0.444451 \tDSC: 0.436799\n",
      "=== Epoch #20 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [07:21<10:18, 19.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 \tTraining Loss: 0.5499 \tValidation Loss: 0.5448\n",
      "Specificity: 0.997269 \tSensitivity: 0.473610 \tF2_score: 0.461550 \tDSC: 0.444646\n",
      "Validation DSC increased.  Saving model ...\n",
      "=== Epoch #21 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 21/51 [07:41<09:59, 19.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 \tTraining Loss: 0.5426 \tValidation Loss: 0.5455\n",
      "Specificity: 0.997063 \tSensitivity: 0.480473 \tF2_score: 0.462698 \tDSC: 0.438730\n",
      "=== Epoch #22 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/51 [08:01<09:39, 19.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 \tTraining Loss: 0.5476 \tValidation Loss: 0.5578\n",
      "Specificity: 0.997462 \tSensitivity: 0.451056 \tF2_score: 0.445537 \tDSC: 0.437582\n",
      "=== Epoch #23 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/51 [08:22<09:25, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \tTraining Loss: 0.5490 \tValidation Loss: 0.5717\n",
      "Specificity: 0.997605 \tSensitivity: 0.429275 \tF2_score: 0.428762 \tDSC: 0.428079\n",
      "=== Epoch #24 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [08:42<09:04, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 \tTraining Loss: 0.5942 \tValidation Loss: 0.5769\n",
      "Specificity: 0.997661 \tSensitivity: 0.421510 \tF2_score: 0.422934 \tDSC: 0.425104\n",
      "=== Epoch #25 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 25/51 [09:03<08:52, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \tTraining Loss: 0.5570 \tValidation Loss: 0.5556\n",
      "Specificity: 0.996972 \tSensitivity: 0.472994 \tF2_score: 0.454722 \tDSC: 0.429855\n",
      "=== Epoch #26 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [09:24<08:29, 20.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 \tTraining Loss: 0.4917 \tValidation Loss: 0.5642\n",
      "Specificity: 0.997200 \tSensitivity: 0.453418 \tF2_score: 0.442238 \tDSC: 0.426505\n",
      "=== Epoch #27 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/51 [09:46<08:22, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 \tTraining Loss: 0.5248 \tValidation Loss: 0.5803\n",
      "Specificity: 0.997579 \tSensitivity: 0.419829 \tF2_score: 0.419667 \tDSC: 0.419476\n",
      "=== Epoch #28 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [10:06<07:59, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 \tTraining Loss: 0.5139 \tValidation Loss: 0.5728\n",
      "Specificity: 0.997379 \tSensitivity: 0.434661 \tF2_score: 0.428959 \tDSC: 0.420808\n",
      "=== Epoch #29 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 29/51 [10:26<07:29, 20.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 \tTraining Loss: 0.5553 \tValidation Loss: 0.5944\n",
      "Specificity: 0.997655 \tSensitivity: 0.401144 \tF2_score: 0.403864 \tDSC: 0.408121\n",
      "=== Epoch #30 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [10:48<07:21, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 \tTraining Loss: 0.5638 \tValidation Loss: 0.5668\n",
      "Specificity: 0.997351 \tSensitivity: 0.441663 \tF2_score: 0.434899 \tDSC: 0.425182\n",
      "=== Epoch #31 ===\n",
      "=== Training ===\n",
      "\n",
      "DEBUG: Data Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "DEBUG: Target Shape: torch.Size([32, 1, 48, 96, 96])\n",
      "\n",
      "DEBUG: Target Max Value: 1\n",
      "âœ… Mask å€¼æ­£å¸¸ (0-1)ã€‚\n",
      "1 / 14...\n",
      "=== Validation ===\n",
      "1 / 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [11:08<06:54, 20.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 \tTraining Loss: 0.4847 \tValidation Loss: 0.5738\n",
      "Specificity: 0.997608 \tSensitivity: 0.425036 \tF2_score: 0.424939 \tDSC: 0.424889\n",
      "=== Epoch #32 ===\n",
      "=== Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 31/51 [11:13<07:14, 21.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸš€ å¼€å§‹è®­ç»ƒ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m CONFIG[\u001b[33m'\u001b[39m\u001b[33minference_only\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# 3. æŠŠ scheduler ä¼ è¿›å»\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     model = \u001b[43mtrain_3D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_on_gpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperformance_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetric_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ä¼ å…¥ scheduler\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# è®­ç»ƒç»“æŸåå¤„ç†æ•°æ®\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(metric_save_path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train.py:176\u001b[39m, in \u001b[36mtrain_3D\u001b[39m\u001b[34m(n_epochs, loaders, model, optimizer, criterion, train_on_gpu, performance_metrics, path, metric_save_path, threshold, scheduler)\u001b[39m\n\u001b[32m    174\u001b[39m model.train()\n\u001b[32m    175\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m=== Training ===\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# move to GPU\u001b[39;49;00m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_on_gpu\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1492\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1290\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# è®¾ç½®ä¿å­˜è·¯å¾„\n",
    "results_dir = 'results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M\")\n",
    "experiment_name = f\"run_{timestamp}\"\n",
    "model_save_path = os.path.join(results_dir, f\"{experiment_name}_model.pt\")\n",
    "loss_plot_path = os.path.join(results_dir, f\"{experiment_name}_loss_curve.png\")\n",
    "metric_save_path = os.path.join(results_dir, f\"{experiment_name}_metrics.csv\")\n",
    "test_save_path = os.path.join(results_dir, f\"{experiment_name}_inference_results.csv\")\n",
    "\n",
    "print(f\"ğŸš€ å¼€å§‹è®­ç»ƒ: {experiment_name}\")\n",
    "\n",
    "if not CONFIG['inference_only']:\n",
    "    # 3. æŠŠ scheduler ä¼ è¿›å»\n",
    "    model = train_3D(CONFIG['n_epochs'], loaders, model, optimizer, criterion, \n",
    "                     CONFIG['train_on_gpu'], performance_metrics, model_save_path,metric_save_path, 0.5, \n",
    "                     scheduler=scheduler) # ä¼ å…¥ scheduler\n",
    "    \n",
    "    # è®­ç»ƒç»“æŸåå¤„ç†æ•°æ®\n",
    "    if os.path.exists(metric_save_path):\n",
    "        df = pd.read_csv(metric_save_path)\n",
    "        \n",
    "        # ç›´æ¥åœ¨ Jupyter ä¸­ç”»å›¾\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(df['epoch'], df['Training Loss'], label='Train')\n",
    "        plt.plot(df['epoch'], df['Validation Loss'], label='Valid')\n",
    "        plt.legend()\n",
    "        plt.title(f'Training Process ({experiment_name})')\n",
    "        \n",
    "        plt.savefig(loss_plot_path)\n",
    "        plt.show() # ä¿å­˜åå†æ˜¾ç¤º        \n",
    "        \n",
    "        print(\"è®­ç»ƒå®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb68c7-72dd-45e6-bc2e-38d955ee43a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- å¼€å§‹æµ‹è¯•é›†è¯„ä¼° ---\")\n",
    "# åŠ è½½åˆšåˆšè®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæˆ–è€…ä½ å¯ä»¥æ‰‹åŠ¨æŒ‡å®šå…¶ä»–è·¯å¾„ï¼‰\n",
    "best_model_path = model_save_path\n",
    "# best_model_path = 'results/run_20251129-1415_model.pt'\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"åŠ è½½æ¨¡å‹æƒé‡: {best_model_path}...\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"æ­£åœ¨æµ‹è¯• {len(part['test'])} ä¸ªæµ‹è¯•é›†ç—…ä¾‹...\")\n",
    "    df_test = get_inference_performance_metrics_3D(model, part['test'], Pancreas_3D_dataset, \n",
    "                                                  CONFIG['batch_size'], CONFIG['train_on_gpu'], \n",
    "                                                  0.5, kw, kh, kc, dw, dh, dc)\n",
    "    print(\"\\nğŸ“Š æµ‹è¯•é›†ç»“æœç»Ÿè®¡:\")\n",
    "    display(df_test.describe()) # Jupyter ç‰¹æœ‰çš„æ¼‚äº®è¡¨æ ¼æ˜¾ç¤º\n",
    "    \n",
    "    df_test.to_csv(test_save_path, index=False)\n",
    "    print(f\"âœ… è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {test_save_path}\")\n",
    "else:\n",
    "    print(\"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb880c-98ff-4549-8ad9-910fce699c48",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from inference import visualize_patient_prediction_3D\n",
    "\n",
    "i=4\n",
    "visualize_patient_prediction_3D(\n",
    "    model=model, \n",
    "    patient=part['train'][i], # å¯è§†åŒ–æµ‹è¯•\n",
    "    Pancreas_3D_dataset=Pancreas_3D_dataset, \n",
    "    batch_size=1, \n",
    "    train_on_gpu=CONFIG['train_on_gpu'],\n",
    "    threshold=0.5, \n",
    "    kw=kw, kh=kh, kc=kc, dw=dw, dh=dh, dc=dc\n",
    ")\n",
    "visualize_patient_prediction_3D(\n",
    "    model=model, \n",
    "    patient=part['valid'][i], # å¯è§†åŒ–æµ‹è¯•\n",
    "    Pancreas_3D_dataset=Pancreas_3D_dataset, \n",
    "    batch_size=1, \n",
    "    train_on_gpu=CONFIG['train_on_gpu'],\n",
    "    threshold=0.5, \n",
    "    kw=kw, kh=kh, kc=kc, dw=dw, dh=dh, dc=dc\n",
    ")\n",
    "visualize_patient_prediction_3D(\n",
    "    model=model, \n",
    "    patient=part['test'][i], # å¯è§†åŒ–æµ‹è¯•\n",
    "    Pancreas_3D_dataset=Pancreas_3D_dataset, \n",
    "    batch_size=1, \n",
    "    train_on_gpu=CONFIG['train_on_gpu'],\n",
    "    threshold=0.5, \n",
    "    kw=kw, kh=kh, kc=kc, dw=dw, dh=dh, dc=dc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942085c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒç»“æŸåè‡ªåŠ¨å…³æœº\n",
    "# !shutdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
