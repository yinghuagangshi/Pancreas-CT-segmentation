import os
import sys
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import random
import shutil
import nibabel as nib
import pydicom as dicomio

import torch
import torch.optim as optim
from torchsummary import summary
# å¦‚æœä¸æƒ³ç”¨ torch_lr_finderï¼Œå¯ä»¥å°†ä¸‹é¢è¿™è¡Œæ³¨é‡Šæ‰ï¼Œå¹¶åœ¨é…ç½®ä¸­æŠŠ lr_find è®¾ä¸º False
try:
    from torch_lr_finder import LRFinder
except ImportError:
    print("æœªæ‰¾åˆ° torch_lr_finderï¼Œå°†è·³è¿‡ LR æœç´¢åŠŸèƒ½ã€‚")

# ================= å¯¼å…¥æœ¬åœ°æ¨¡å— =================
# ç¡®ä¿ dataset.py, net.py ç­‰æ–‡ä»¶å°±åœ¨åŒä¸€çº§ç›®å½•ä¸‹
try:
    from loss import TverskyLoss
    from net import UNet_2D, UNet_3D
    from volume_patch_composer import volume_composer, patch_creator
    from dataset import Pancreas_2D_dataset, Pancreas_3D_dataset, partitioning
    from metrics import performance_metrics
    from train import train_2D, train_3D
    from inference import (get_inference_performance_metrics_3D)
except ImportError as e:
    print(f"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„æ¨¡å—æ–‡ä»¶ã€‚\nè¯¦ç»†ä¿¡æ¯: {e}")
    print("è¯·ç¡®ä¿ dataset.py, net.py, loss.py ç­‰æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸­ã€‚")
    sys.exit(1)

# ================= âš™ï¸ é…ç½®åŒºåŸŸ (æ ¹æ®ä½ çš„æˆªå›¾è°ƒæ•´) =================
CONFIG = {
    # åŸå§‹æ•°æ®è·¯å¾„ (æ ¹æ®æˆªå›¾)
    'raw_ct_dir': './Pancreas-CT',              # å­˜æ”¾ DICOM çš„æ–‡ä»¶å¤¹
    'raw_label_dir': './Pancreas-CT-Label',     # å­˜æ”¾ .nii.gz çš„æ–‡ä»¶å¤¹
    
    # é¢„å¤„ç†è¾“å‡ºè·¯å¾„ (è„šæœ¬è‡ªåŠ¨ç”Ÿæˆ)
    'processed_2d_dir': './data',               # è½¬æ¢åçš„ PNG å­˜æ”¾å¤„
    'processed_3d_dir': './data3D',             # Resize åçš„ Tensor å­˜æ”¾å¤„
    
    # è®­ç»ƒå‚æ•°
    'unet_2d': False,              # é»˜è®¤ä¸º 3D åˆ†å‰²
    'batch_size': 2,               # æœ¬åœ°æ˜¾å­˜é€šå¸¸è¾ƒå°ï¼Œå»ºè®®è®¾ä¸º 2 æˆ– 4
    'num_workers': 0,              # Windows ä¸‹å»ºè®®è®¾ä¸º 0ï¼Œé¿å…å¤šè¿›ç¨‹æŠ¥é”™
    'n_epochs': 1,                 # æ¼”ç¤ºç”¨ 1 ä¸ª epochï¼Œå®é™…è®­ç»ƒå¯æ”¹ä¸º 50+
    'lr_find': False,
    'inference_only': False,       # å¦‚æœåªæƒ³æµ‹è¯•ï¼Œè®¾ä¸º True
    'train_on_gpu': torch.cuda.is_available(),
    'seed': 51
}

# ================= ğŸ› ï¸ å·¥å…·å‡½æ•° =================

def set_seed(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = True

def prepare_directories():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    for p in [CONFIG['processed_2d_dir'], CONFIG['processed_3d_dir']]:
        if not os.path.exists(p):
            os.makedirs(p)
            print(f"åˆ›å»ºç›®å½•: {p}")

def preprocess_data():
    """
    æ ¸å¿ƒé¢„å¤„ç†é€»è¾‘ï¼š
    1. è¯»å– Pancreas-CT-Label ä¸­çš„ .nii.gz -> è½¬ä¸º PNG å­˜å…¥ ./data/PatientXXX/Masks
    2. è¯»å– Pancreas-CT ä¸­çš„ DICOM -> è½¬ä¸º PNG å­˜å…¥ ./data/PatientXXX/CT
    """
    print("--- å¼€å§‹æ•°æ®é¢„å¤„ç† ---")
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ (æ£€æŸ¥ Patient0001 æ˜¯å¦å­˜åœ¨)
    check_path = os.path.join(CONFIG['processed_2d_dir'], 'Patient0001', 'CT')
    if os.path.exists(check_path) and len(os.listdir(check_path)) > 0:
        print("æ£€æµ‹åˆ° ./data ç›®å½•å·²æœ‰æ•°æ®ï¼Œè·³è¿‡ PNG è½¬æ¢æ­¥éª¤ã€‚")
        return

    # åˆå§‹åŒ–ç—…äººæ–‡ä»¶å¤¹
    for i in range(1, 83):
        patient_id = '{:04d}'.format(i)
        p_folder = os.path.join(CONFIG['processed_2d_dir'], 'Patient' + patient_id)
        os.makedirs(os.path.join(p_folder, 'Masks'), exist_ok=True)
        os.makedirs(os.path.join(p_folder, 'CT'), exist_ok=True)

    # 1. å¤„ç† Masks (NIfTI -> PNG)
    print("æ­£åœ¨å¤„ç† Masks (NIfTI -> PNG)...")
    for i in range(1, 83):
        patient_id = '{:04d}'.format(i)
        # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º label0001.nii.gz
        nifti_filename = f"label{patient_id}.nii.gz"
        nifti_path = os.path.join(CONFIG['raw_label_dir'], nifti_filename)
        
        if not os.path.exists(nifti_path):
            print(f"âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ°æ ‡ç­¾æ–‡ä»¶ {nifti_path}")
            continue

        try:
            img = nib.load(nifti_path)
            img_data = img.get_fdata()
            
            # ä¿å­˜æ¯ä¸€å±‚åˆ‡ç‰‡
            for s in range(img_data.shape[2]):
                slice_label = '{:03d}'.format(s + 1)
                slice_img = img_data[:, :, s]
                save_path = os.path.join(CONFIG['processed_2d_dir'], 'Patient' + patient_id, 
                                         'Masks', f"M_{slice_label}.png")
                cv2.imwrite(save_path, slice_img)
        except Exception as e:
            print(f"å¤„ç† Mask {patient_id} å‡ºé”™: {e}")

    # 2. å¤„ç† CT (DICOM -> PNG)
    print("æ­£åœ¨å¤„ç† CT (DICOM -> PNG)... è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ")
    for i in range(1, 83):
        patient_id = '{:04d}'.format(i)
        # æœç´¢ DICOM æ–‡ä»¶ï¼Œç»“æ„é€šå¸¸æ˜¯ Pancreas-CT/PANCREAS_0001/.../*.dcm
        # ä½¿ç”¨ recursive=True æ¥ç©¿é€å¤šå±‚å­æ–‡ä»¶å¤¹
        search_pattern = os.path.join(CONFIG['raw_ct_dir'], f"PANCREAS_{patient_id}", "**", "*.dcm")
        dcm_files = glob.glob(search_pattern, recursive=True)

        if not dcm_files:
            print(f"âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ° Patient {patient_id} çš„ DICOM æ–‡ä»¶")
            continue

        for f in dcm_files:
            try:
                # æ–‡ä»¶åé€šå¸¸åŒ…å«åˆ‡ç‰‡åºå·ï¼Œä¾‹å¦‚ 1-001.dcm
                file_name = os.path.basename(f)
                # å°è¯•æå–ä¸­é—´çš„æ•°å­—éƒ¨åˆ†ä½œä¸ºåºå·
                parts = file_name.replace('.dcm', '').split('-')
                if len(parts) > 1:
                    slice_idx = parts[-1] 
                else:
                    slice_idx = parts[0] # fallback

                save_path = os.path.join(CONFIG['processed_2d_dir'], 'Patient' + patient_id, 
                                         'CT', f"CT_{slice_idx}.png")
                
                dcm = dicomio.read_file(f)
                img_array = dcm.pixel_array
                # æ ¹æ®åŸä»£ç é€»è¾‘ï¼Œéœ€è¦è½¬ç½® (Transpose)
                cv2.imwrite(save_path, img_array.transpose(1, 0))
            except Exception as e:
                pass # å¿½ç•¥å•ä¸ªæ–‡ä»¶é”™è¯¯
    print("æ•°æ®è½¬æ¢å®Œæˆã€‚")

# ================= ğŸš€ ä¸»ç¨‹åºé€»è¾‘ =================

def main():
    set_seed(CONFIG['seed'])
    prepare_directories()
    
    print(f"CUDA æ˜¯å¦å¯ç”¨: {CONFIG['train_on_gpu']}")
    if CONFIG['train_on_gpu']:
        print(f"ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)}")

    # 1. æ‰§è¡Œæ•°æ®é¢„å¤„ç†
    preprocess_data()

    # 2. æ„å»ºæ•°æ®ç´¢å¼•å­—å…¸ (è¿™æ˜¯ volume_composer éœ€è¦çš„æ ¼å¼)
    print("æ„å»ºæ–‡ä»¶ç´¢å¼•...")
    patient_path_list = {'CT': {}, 'Masks': {}}
    patient_image_cnt_CT = {}
    patient_image_cnt_Mask = {}

    valid_patients = []
    # æ‰«æç”Ÿæˆçš„ data ç›®å½•
    patient_dirs = sorted(glob.glob(os.path.join(CONFIG['processed_2d_dir'], 'Patient*')))
    
    for p_dir in patient_dirs:
        p_key = os.path.basename(p_dir) # e.g., "Patient0001"
        
        ct_files = sorted(glob.glob(os.path.join(p_dir, 'CT', '*.png')))
        mask_files = sorted(glob.glob(os.path.join(p_dir, 'Masks', '*.png')))
        
        if len(ct_files) > 0 and len(ct_files) == len(mask_files):
            patient_path_list['CT'][p_key] = ct_files
            patient_path_list['Masks'][p_key] = mask_files
            patient_image_cnt_CT[p_key] = len(ct_files)
            patient_image_cnt_Mask[p_key] = len(mask_files)
            valid_patients.append(p_key)
        else:
            # print(f"è·³è¿‡ä¸å®Œæ•´æ•°æ®: {p_key} (CT: {len(ct_files)}, Mask: {len(mask_files)})")
            pass

    print(f"æœ‰æ•ˆç—…ä¾‹æ•°: {len(valid_patients)}")

    # 3. ä½“ç§¯é‡é‡‡æ · (Volume Resize -> 3D Tensor)
    print("æ‰§è¡Œ 3D ä½“ç§¯é‡é‡‡æ · (ç”Ÿæˆ .pt æ–‡ä»¶)...")
    d1 = torch.linspace(-1, 1, 256)
    d2 = torch.linspace(-1, 1, 256)
    d3 = torch.linspace(-1, 1, 128)
    meshx, meshy, meshz = torch.meshgrid((d1, d2, d3), indexing='ij')
    grid = torch.stack((meshx, meshy, meshz), 3).unsqueeze(0)

    # è°ƒç”¨ volume_patch_composer.py ä¸­çš„å‡½æ•°
    # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿ volume_patch_composer.py é‡Œçš„ä¿å­˜è·¯å¾„ä¹Ÿæ˜¯æŒ‡å‘ CONFIG['processed_3d_dir']
    # å¦‚æœåŸæ–‡ä»¶å†™æ­»äº† '/content/data3D/'ï¼Œéœ€è¦ä½ æ‰‹åŠ¨å»æ”¹ä¸€ä¸‹é‚£ä¸ªæ–‡ä»¶ï¼Œæˆ–è€…æˆ‘ä»¬è¿™é‡Œ monkey patch ä¸€ä¸‹
    # è¿™é‡Œå‡è®¾æˆ‘ä»¬ä¼ é€’æ­£ç¡®çš„å­—å…¸è¿›å»
    
    # ä¸ºäº†é¿å…ä¿®æ”¹ volume_patch_composer.pyï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ‰‹åŠ¨æ£€æŸ¥å¹¶ç”Ÿæˆ
    # åŸå‡½æ•° volume_composer å†…éƒ¨è·¯å¾„å¯èƒ½å†™æ­»äº†ï¼Œå»ºè®®å»ä¿®æ”¹ volume_patch_composer.py:
    # å°† '/content/data3D/' æ›¿æ¢ä¸º './data3D/'
    
    for patient in valid_patients:
        out_ct_path = os.path.join(CONFIG['processed_3d_dir'], patient + '_CT.pt')
        if not os.path.exists(out_ct_path):
            try:
                # å°è¯•è°ƒç”¨ï¼Œå¦‚æœ volume_patch_composer å†…éƒ¨å†™æ­»äº†è·¯å¾„å¯èƒ½ä¼šå­˜é”™åœ°æ–¹
                # å»ºè®®æ‰“å¼€ volume_patch_composer.py æŠŠæ‰€æœ‰ /content/data3D æ”¹ä¸º ./data3D
                volume_composer(patient, patient_image_cnt_CT, patient_path_list, grid)
                
                # Hack: å¦‚æœå®ƒå­˜åˆ°äº†é»˜è®¤ä½ç½® (ä¾‹å¦‚æ ¹ç›®å½•)ï¼Œç§»åŠ¨å®ƒ
                if os.path.exists(f'/content/data3D/{patient}_CT.pt'):
                    shutil.move(f'/content/data3D/{patient}_CT.pt', out_ct_path)
            except Exception as e:
                print(f"Resizing {patient} error: {e}")
                # å¯ä»¥åœ¨è¿™é‡Œé‡å†™ç®€å•çš„ resize é€»è¾‘ï¼Œä½†ä¸ºäº†åˆ©ç”¨åŸä»£ç æš‚ä¸”å¦‚æ­¤

    # æ£€æŸ¥ data3D æ–‡ä»¶å¤¹æ˜¯å¦æœ‰å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰ï¼Œæç¤ºç”¨æˆ·ä¿®æ”¹ volume_patch_composer.py
    if not os.listdir(CONFIG['processed_3d_dir']):
        print("âŒ è­¦å‘Š: data3D æ–‡ä»¶å¤¹ä¸ºç©ºã€‚")
        print("è¯·æ‰“å¼€ 'volume_patch_composer.py' æ–‡ä»¶ï¼Œå°†é‡Œé¢æ‰€æœ‰çš„ '/content/data3D/' æ›¿æ¢ä¸º './data3D/'ï¼Œç„¶åé‡æ–°è¿è¡Œã€‚")
        return

    # 4. æ•°æ®åˆ’åˆ†ä¸åŠ è½½
    print("å‡†å¤‡ Dataset å’Œ DataLoader...")
    part = partitioning(valid_patients, split_ratio=[0.7, 0.1, 0.2])

    # 3D å‚æ•°
    kc, kh, kw = 32, 64, 64
    dc, dh, dw = 32, 64, 64

    CT_patches = {}
    mask_patches = {}
    
    # åŒæ ·ï¼Œpatch_creator å†…éƒ¨å¯èƒ½ä¹Ÿæœ‰è·¯å¾„ç¡¬ç¼–ç ï¼Œè¯·æ£€æŸ¥ volume_patch_composer.py
    for p in ['train', 'valid']:
        CT_patches[p], mask_patches[p] = patch_creator(part[p], kw, kh, kc, dw, dh, dc)

    dataset_train = Pancreas_3D_dataset(CT_patches['train'], mask_patches['train'], augment=True)
    dataset_valid = Pancreas_3D_dataset(CT_patches['valid'], mask_patches['valid'], augment=False)

    loaders = {
        'train': torch.utils.data.DataLoader(dataset_train, batch_size=CONFIG['batch_size'], 
                                             shuffle=True, num_workers=CONFIG['num_workers']),
        'valid': torch.utils.data.DataLoader(dataset_valid, batch_size=CONFIG['batch_size'], 
                                             shuffle=False, num_workers=CONFIG['num_workers'])
    }

    # 5. æ¨¡å‹ä¸è®­ç»ƒ
    print("åˆå§‹åŒ– 3D UNet æ¨¡å‹...")
    model = UNet_3D(1, 1, 32, 0.2)
    if CONFIG['train_on_gpu']:
        model.cuda()

    criterion = TverskyLoss(1e-8, 0.3, 0.7)
    optimizer = optim.Adam(model.parameters(), lr=0.005)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.08, 
                                                    steps_per_epoch=len(loaders['train']), 
                                                    epochs=CONFIG['n_epochs'])

    if not CONFIG['inference_only']:
        print(f"å¼€å§‹è®­ç»ƒ ({CONFIG['n_epochs']} epochs)...")
        # è°ƒç”¨ train.py ä¸­çš„ train_3D
        model = train_3D(CONFIG['n_epochs'], loaders, model, optimizer, criterion, 
                         CONFIG['train_on_gpu'], performance_metrics, 'model.pt', 0.5)
        
        # ä¿å­˜ Loss æ›²çº¿
        if os.path.exists('performance_metrics.csv'):
            df = pd.read_csv('performance_metrics.csv')
            plt.figure()
            plt.plot(df['epoch'], df['Training Loss'], label='Train')
            plt.plot(df['epoch'], df['Validation Loss'], label='Valid')
            plt.legend()
            plt.title('Training Process')
            plt.savefig('loss_curve.png')
            print("è®­ç»ƒå®Œæˆï¼ŒLoss æ›²çº¿å·²ä¿å­˜ä¸º loss_curve.png")

    # 6. æ¨ç†æµ‹è¯•
    print("å¼€å§‹æµ‹è¯•é›†æ¨ç†...")
    if os.path.exists('model.pt'):
        model.load_state_dict(torch.load('model.pt'))
    
    # åŒæ ·éœ€è¦æ³¨æ„ inference.py å†…éƒ¨æ˜¯å¦ä¹Ÿæœ‰è·¯å¾„ç¡¬ç¼–ç 
    df = get_inference_performance_metrics_3D(model, part['test'], Pancreas_3D_dataset, 
                                              CONFIG['batch_size'], CONFIG['train_on_gpu'], 
                                              0.5, kw, kh, kc, dw, dh, dc)
    print("\næµ‹è¯•ç»“æœç»Ÿè®¡:")
    print(df.describe())
    df.to_csv('inference_results.csv')

if __name__ == '__main__':
    main()