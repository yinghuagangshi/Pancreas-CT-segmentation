import os
import sys
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import random
import shutil
import nibabel as nib
import pydicom as dicomio  # pydicom åº“

import torch
import torch.optim as optim

# å°è¯•å¯¼å…¥è¾…åŠ©æ¨¡å—
try:
    from loss import TverskyLoss, MixedLoss
    from net import UNet_2D, UNet_3D
    from volume_patch_composer import volume_composer, patch_creator
    from dataset import Pancreas_2D_dataset, Pancreas_3D_dataset, partitioning
    from metrics import performance_metrics
    from train import train_2D, train_3D
    from inference import (get_inference_performance_metrics_3D)
except ImportError as e:
    print(f"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„æ¨¡å—æ–‡ä»¶ã€‚\nè¯¦ç»†ä¿¡æ¯: {e}")
    sys.exit(1)

def process_ct_window(ct_array, w_level=40, w_width=400):
    """
    å¯¹ CT æ•°æ®è¿›è¡Œçª—å®½çª—ä½è°ƒæ•´å’Œå½’ä¸€åŒ–ã€‚
    èƒ°è…º/è½¯ç»„ç»‡æ¨è: WL=40, WW=350~400
    """
    # 1. åº”ç”¨çª—å®½çª—ä½
    min_val = w_level - w_width / 2
    max_val = w_level + w_width / 2
    
    ct_clipped = np.clip(ct_array, min_val, max_val)
    
    # 2. å½’ä¸€åŒ–åˆ° [0, 255]
    ct_norm = (ct_clipped - min_val) / (max_val - min_val)
    ct_norm = ct_norm * 255.0
    
    return ct_norm.astype(np.uint8)


# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
CONFIG = {
    'raw_ct_dir': './Pancreas-CT',              
    'raw_label_dir': './Pancreas-CT-Label',     
    'processed_2d_dir': './data',               
    'processed_3d_dir': './data3D',             
    
    'unet_2d': False,              
    'batch_size': 4,               
    'num_workers': 0,              
    'n_epochs': 50,                # ğŸš€ ä¿®æ”¹ï¼šæ­£å¼è®­ç»ƒå»ºè®®è®¾ä¸º 50ã€‚å¦‚æœæƒ³å¿«é€Ÿæµ‹è¯•ï¼Œå¯æ”¹å› 1 æˆ– 5
    'inference_only': False,       
    'train_on_gpu': torch.cuda.is_available(),
    'seed': 51
}

# ================= ğŸ› ï¸ å·¥å…·å‡½æ•° =================

def set_seed(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = True

def prepare_directories():
    for p in [CONFIG['processed_2d_dir'], CONFIG['processed_3d_dir']]:
        if not os.path.exists(p):
            os.makedirs(p)

def preprocess_data_robust():
    """
    é²æ£’çš„æ•°æ®é¢„å¤„ç†å‡½æ•° (v4 - æ™ºèƒ½ç¼“å­˜ç‰ˆ)ï¼š
    å¦‚æœæ£€æµ‹åˆ°æ•°æ®å·²å­˜åœ¨ï¼Œç›´æ¥è·³è¿‡è€—æ—¶çš„ç”Ÿæˆæ­¥éª¤ã€‚
    """
    print("--- æ£€æŸ¥æ•°æ®çŠ¶æ€ ---")
    
    # ğŸš€ ä¼˜åŒ–ç‚¹ 1ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®ï¼Œæœ‰åˆ™è·³è¿‡
    # æ£€æŸ¥æœ€åä¸€ä¸ªç—…äººæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º
    check_patient = os.path.join(CONFIG['processed_2d_dir'], 'Patient0082', 'CT')
    if os.path.exists(check_patient) and len(os.listdir(check_patient)) > 0:
        print("âœ… æ£€æµ‹åˆ°æœ¬åœ°å·²æœ‰é¢„å¤„ç†æ•°æ® (./data)ï¼Œè·³è¿‡ PNG ç”Ÿæˆæ­¥éª¤ã€‚")
        return

    print("ğŸ”„ æœªæ‰¾åˆ°å®Œæ•´æ•°æ®ï¼Œå¼€å§‹æ‰§è¡Œé¢„å¤„ç† (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    prepare_directories()

    # æ£€æŸ¥ pydicom ç‰ˆæœ¬å…¼å®¹æ€§
    try:
        if not hasattr(dicomio, 'dcmread'):
            dicomio.dcmread = dicomio.read_file
    except:
        pass

    for i in range(1, 83):
        patient_id = '{:04d}'.format(i)
        
        # è·¯å¾„å‡†å¤‡
        nifti_filename = f"label{patient_id}.nii.gz"
        nifti_path = os.path.join(CONFIG['raw_label_dir'], nifti_filename)
        ct_folder_pattern = os.path.join(CONFIG['raw_ct_dir'], f"PANCREAS_{patient_id}", "**", "*.dcm")
        
        # 1. æ£€æŸ¥æºæ–‡ä»¶
        if not os.path.exists(nifti_path):
            # print(f"âš ï¸  [Patient {patient_id}] è·³è¿‡: æ‰¾ä¸åˆ°æ ‡ç­¾æ–‡ä»¶")
            continue
        
        dcm_files = glob.glob(ct_folder_pattern, recursive=True)
        if not dcm_files:
            # print(f"âš ï¸  [Patient {patient_id}] è·³è¿‡: æ‰¾ä¸åˆ° DICOM æ–‡ä»¶")
            continue

        # 2. è¯»å–å¹¶æ’åº DICOM
        try:
            slices = []
            for f in dcm_files:
                try:
                    ds = dicomio.dcmread(f)

                    # å…ˆè½¬ä¸º float é¿å…è®¡ç®—æº¢å‡º
                    image = ds.pixel_array.astype(np.float32)
                    
                    # åº”ç”¨æ–œç‡å’Œæˆªè· (å¦‚æœå­˜åœ¨)
                    if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):
                        slope = float(ds.RescaleSlope)
                        intercept = float(ds.RescaleIntercept)
                        image = image * slope + intercept

                    # slices.append(ds)
                    slices.append((float(ds.ImagePositionPatient[2]), image))
                except Exception:
                    pass
            
            if not slices:
                continue

            # æŒ‰ Z è½´ä½ç½®æ’åº
            # slices.sort(key=lambda x: float(x.ImagePositionPatient[2]))
            slices.sort(key=lambda x: x[0])
            
        except Exception as e:
            print(f"âŒ [Patient {patient_id}] å¤„ç†å´©æºƒ: {e}")
            continue

        # 3. è¯»å– Mask
        try:
            mask_obj = nib.load(nifti_path)
            mask_data = mask_obj.get_fdata()
        except Exception as e:
            print(f"âŒ [Patient {patient_id}] NIfTI è¯»å–å¤±è´¥: {e}")
            continue

        # 4. å¯¹é½å±‚æ•°
        num_dcm = len(slices)
        num_mask = mask_data.shape[2]
        valid_slices = min(num_dcm, num_mask)
        
        if valid_slices < 10:
            continue
            
        # 5. ä¿å­˜ PNG
        save_dir_ct = os.path.join(CONFIG['processed_2d_dir'], 'Patient' + patient_id, 'CT')
        save_dir_mask = os.path.join(CONFIG['processed_2d_dir'], 'Patient' + patient_id, 'Masks')
        os.makedirs(save_dir_ct, exist_ok=True)
        os.makedirs(save_dir_mask, exist_ok=True)

        # (å‰é¢çš„ä»£ç ä¿æŒä¸å˜)
        try:
            for s in range(valid_slices):
                mask_slice = mask_data[:, :, s]
                
                # è·å–åŸå§‹ CT æ•°æ®
                # raw_ct_slice = slices[s].pixel_array.transpose(1, 0)
                raw_ct_slice = slices[s][1].transpose(1, 0)
                
                # --- ğŸ”¥ ä¿®æ”¹å¼€å§‹ ğŸ”¥ ---
                # 1. å¯¹ CT è¿›è¡Œçª—ä½è°ƒæ•´å’Œå½’ä¸€åŒ– (å…³é”®ä¿®å¤!)
                processed_ct_slice = process_ct_window(raw_ct_slice, w_level=40, w_width=400)
                
                # 2. ç¡®ä¿ Mask ä¹Ÿæ˜¯ uint8 æ ¼å¼ (0 å’Œ 255, æˆ–è€… 0 å’Œ 1)
                # å»ºè®®å°† Mask ä¹˜ä»¥ 255 ä»¥ä¾¿è‚‰çœ¼è§‚å¯Ÿï¼Œä½†åœ¨è¯»å–æ—¶è¦é™¤å›æ¥
                mask_slice = (mask_slice * 255).astype(np.uint8)
                # è¿™é‡Œä¸ºäº†å…¼å®¹ä½ ç°æœ‰çš„ dataset ä»£ç (å‡è®¾å®ƒè¯»å–0/1)ï¼Œæˆ‘ä»¬ä¿æŒ 0/1 ä½†è½¬ä¸º uint8
                # mask_slice = mask_slice.astype(np.uint8)
                
                # --- ğŸ”¥ ä¿®æ”¹ç»“æŸ ğŸ”¥ ---

                filename = f"{s:04d}.png"
                cv2.imwrite(os.path.join(save_dir_mask, filename), mask_slice)
                cv2.imwrite(os.path.join(save_dir_ct, filename), processed_ct_slice)
            
        except Exception as e:
            print(f"âŒ [Patient {patient_id}] ä¿å­˜å‡ºé”™: {e}")      

    print("--- æ•°æ®é¢„å¤„ç†å®Œæˆ ---")

def main():
    set_seed(CONFIG['seed'])
    
    # ================= ğŸ“ 1. è®¾ç½®ç»“æœç›®å½•å’Œæ—¶é—´æˆ³ (ä¿®æ”¹éƒ¨åˆ†) =================
    # åˆ›å»º results æ–‡ä»¶å¤¹
    results_dir = 'results'
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)

    # ç”Ÿæˆæ—¶é—´æˆ³ï¼Œä¾‹å¦‚: "20251126-1030"
    import time
    timestamp = time.strftime("%Y%m%d-%H%M")
    experiment_name = f"run_{timestamp}"
    
    print(f"ğŸš€ æœ¬æ¬¡å®éªŒID: {experiment_name}")
    print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜åœ¨: {results_dir}/")

    # å®šä¹‰å¸¦è·¯å¾„çš„ä¿å­˜æ–‡ä»¶å
    model_save_path = os.path.join(results_dir, f"{experiment_name}_model.pt")
    loss_plot_path = os.path.join(results_dir, f"{experiment_name}_loss_curve.png")
    metric_save_path = os.path.join(results_dir, f"{experiment_name}_metrics.csv")
    test_save_path = os.path.join(results_dir, f"{experiment_name}_inference_results.csv")
    # ====================================================================

    print(f"CUDA æ˜¯å¦å¯ç”¨: {CONFIG['train_on_gpu']}")
    if CONFIG['train_on_gpu']:
        print(f"ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)}")

    # 1. æ™ºèƒ½é¢„å¤„ç†
    preprocess_data_robust()

    # 2. æ„å»ºæ•°æ®ç´¢å¼•
    print("æ„å»ºæ–‡ä»¶ç´¢å¼•...")
    patient_path_list = {'CT': {}, 'Masks': {}}
    patient_image_cnt_CT = {}
    patient_image_cnt_Mask = {}

    valid_patients = []
    patient_dirs = sorted(glob.glob(os.path.join(CONFIG['processed_2d_dir'], 'Patient*')))
    
    for p_dir in patient_dirs:
        p_key = os.path.basename(p_dir)
        ct_files = sorted(glob.glob(os.path.join(p_dir, 'CT', '*.png')))
        mask_files = sorted(glob.glob(os.path.join(p_dir, 'Masks', '*.png')))
        
        if len(ct_files) > 0 and len(ct_files) == len(mask_files):
            patient_path_list['CT'][p_key] = ct_files
            patient_path_list['Masks'][p_key] = mask_files
            patient_image_cnt_CT[p_key] = len(ct_files)
            patient_image_cnt_Mask[p_key] = len(mask_files)
            valid_patients.append(p_key)

    print(f"æœ‰æ•ˆç—…ä¾‹æ•°: {len(valid_patients)}")
    if len(valid_patients) == 0:
        print("âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆç—…ä¾‹ã€‚è¯·æ£€æŸ¥æ•°æ®ã€‚")
        return

    # 3. ä½“ç§¯é‡é‡‡æ · (æ™ºèƒ½è·³è¿‡)
    print("æ£€æŸ¥ 3D æ•°æ®ç¼“å­˜...")
    d1 = torch.linspace(-1, 1, 256)
    d2 = torch.linspace(-1, 1, 256)
    d3 = torch.linspace(-1, 1, 128)
    meshx, meshy, meshz = torch.meshgrid((d1, d2, d3), indexing='ij')
    grid = torch.stack((meshx, meshy, meshz), 3).unsqueeze(0)

    new_pt_count = 0
    for patient in valid_patients:
        out_ct_path = os.path.join(CONFIG['processed_3d_dir'], patient + '_CT.pt')
        if not os.path.exists(out_ct_path):
            try:
                volume_composer(patient, patient_image_cnt_CT, patient_path_list, grid)
                new_pt_count += 1
            except Exception as e:
                print(f"Resizing {patient} error: {e}")
    
    if new_pt_count == 0:
        print("âœ… æ‰€æœ‰ 3D æ•°æ® (.pt) å·²å­˜åœ¨ï¼Œè·³è¿‡é‡é‡‡æ ·æ­¥éª¤ã€‚")
    else:
        print(f"ğŸ”„ æ–°ç”Ÿæˆäº† {new_pt_count} ä¸ª 3D æ•°æ®æ–‡ä»¶ã€‚")

    # 4. è®­ç»ƒå‡†å¤‡
    print("å‡†å¤‡ Dataset...")
    part = partitioning(valid_patients, split_ratio=[0.7, 0.1, 0.2])
    
    # kc: Kernel Depth (åˆ‡å—çš„æ·±åº¦/å±‚æ•°)
    # kh: Kernel Height (åˆ‡å—çš„é«˜åº¦)
    # kw: Kernel Width (åˆ‡å—çš„å®½åº¦)
    kc, kh, kw = 32, 64, 64
    # dc, dh, dw: Stride (æ»‘åŠ¨çª—å£çš„æ­¥é•¿ï¼Œé€šå¸¸è®¾ä¸ºå’Œä¸Šé¢ä¸€æ ·ï¼Œè¡¨ç¤ºä¸é‡å )
    dc, dh, dw = 32, 64, 64

    CT_patches = {}
    mask_patches = {}
    
    print("åŠ è½½ Patches (è¿™æ­¥éœ€è¦ä¸€ç‚¹å†…å­˜)...")
    for p in ['train', 'valid']:
        CT_patches[p], mask_patches[p] = patch_creator(part[p], kw, kh, kc, dw, dh, dc)

    dataset_train = Pancreas_3D_dataset(CT_patches['train'], mask_patches['train'], augment=True, is_train=True)
    dataset_valid = Pancreas_3D_dataset(CT_patches['valid'], mask_patches['valid'], augment=False , is_train=False)

    loaders = {
        'train': torch.utils.data.DataLoader(dataset_train, batch_size=CONFIG['batch_size'], 
                                             shuffle=True, num_workers=CONFIG['num_workers']),
        'valid': torch.utils.data.DataLoader(dataset_valid, batch_size=CONFIG['batch_size'], 
                                             shuffle=False, num_workers=CONFIG['num_workers'])
    }

    # 5. æ¨¡å‹è®­ç»ƒ
    print("åˆå§‹åŒ–æ¨¡å‹...")
    model = UNet_3D(1, 1, 32, 0.2)
    if CONFIG['train_on_gpu']:
        model.cuda()

    # ä¿®æ”¹ä¸ºä½ å®é™…çš„æ¨¡å‹æ–‡ä»¶å
    checkpoint_path = './results/run_20251126-1659_model.pt' 
    
    if os.path.exists(checkpoint_path):
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {checkpoint_path}")
        # åŠ è½½æƒé‡
        model.load_state_dict(torch.load(checkpoint_path))
        print("âœ… åŠ è½½æˆåŠŸï¼å°†åœ¨ç°æœ‰åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒã€‚")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")   

    # âœ… ä½¿ç”¨æ–°çš„æ··åˆ Loss
    # alpha=0.7 å¼ºè°ƒå¬å›ï¼Œbce_weight=0.5 æä¾›æ¢¯åº¦å¹³æ»‘
    # criterion = MixedLoss(alpha=0.7, beta=0.3, bce_weight=0.5) 

    criterion = TverskyLoss(1e-6, 0.7, 0.3)
    # 1. å®šä¹‰åŸºç¡€ä¼˜åŒ–å™¨ (LR ä¼šè¢« Scheduler è¦†ç›–ï¼Œæ‰€ä»¥è¿™é‡Œåˆå§‹ LR å¯ä»¥éšæ„ï¼Œä½†å»ºè®®è®¾ä¸º max_lr çš„ 1/10 æˆ– 1/25)
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    # 2.å®šä¹‰ OneCycleLR
    # max_lr: æœ€å¤§å­¦ä¹ ç‡ï¼Œå¯ä»¥å°è¯• 1e-3 æˆ– 5e-4
    # steps_per_epoch: æ¯ä¸ª epoch çš„ batch æ•°é‡
    # epochs: æ€» epoch æ•°
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer, 
        max_lr=1e-3, 
        steps_per_epoch=len(loaders['train']), 
        epochs=CONFIG['n_epochs']
    )
    
    if len(loaders['train']) == 0:
        print("âŒ è®­ç»ƒé›†ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒã€‚")
        return

    if not CONFIG['inference_only']:
        print(f"å¼€å§‹è®­ç»ƒ ({CONFIG['n_epochs']} epochs)...")

        # 3. æŠŠ scheduler ä¼ è¿›å»
        model = train_3D(CONFIG['n_epochs'], loaders, model, optimizer, criterion, 
                         CONFIG['train_on_gpu'], performance_metrics, model_save_path,metric_save_path, 0.5, 
                         scheduler=scheduler) # ä¼ å…¥ scheduler       
        
        # å¤„ç† Loss æ›²çº¿å’Œ Metrics
        if os.path.exists(metric_save_path):
            try:
                df = pd.read_csv(metric_save_path)
                
                # ç»˜å›¾å¹¶ä¿å­˜åˆ° results æ–‡ä»¶å¤¹
                plt.figure()
                plt.plot(df['epoch'], df['Training Loss'], label='Train')
                plt.plot(df['epoch'], df['Validation Loss'], label='Valid')
                plt.legend()
                plt.title(f'Training Process ({experiment_name})')
                plt.savefig(loss_plot_path) # ä¿®æ”¹ä¿å­˜è·¯å¾„
                print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜: {loss_plot_path}")
                plt.close() # å…³é—­å›¾è¡¨é‡Šæ”¾å†…å­˜
                
            except Exception as e:
                print(f"ä¿å­˜æ›²çº¿å‡ºé”™: {e}")

    # 6. æµ‹è¯•é›†æ¨ç† (Evaluation)
    print("\n--- å¼€å§‹æµ‹è¯•é›†è¯„ä¼° ---")
    # ä¿®æ”¹ï¼šä»æ–°çš„ model_save_path åŠ è½½æ¨¡å‹
    if os.path.exists(model_save_path):
        print(f"åŠ è½½æ¨¡å‹æƒé‡: {model_save_path}...")
        model.load_state_dict(torch.load(model_save_path))
        model.eval()
        
        print(f"æ­£åœ¨æµ‹è¯• {len(part['test'])} ä¸ªæµ‹è¯•é›†ç—…ä¾‹...")
        df_test = get_inference_performance_metrics_3D(model, part['test'], Pancreas_3D_dataset, 
                                                  CONFIG['batch_size'], CONFIG['train_on_gpu'], 
                                                  0.5, kw, kh, kc, dw, dh, dc)
        print("\nğŸ“Š æµ‹è¯•é›†ç»“æœç»Ÿè®¡:")
        print(df_test.describe())
        
        # ä¿å­˜åˆ° results æ–‡ä»¶å¤¹
        df_test.to_csv(test_save_path, index=False)
        print(f"âœ… è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {test_save_path}")

    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {model_save_path}ï¼Œè·³è¿‡æµ‹è¯•ã€‚")

    print("è„šæœ¬å…¨éƒ¨è¿è¡Œç»“æŸã€‚")

if __name__ == '__main__':
    main() # python -u "e:\Pancreas-CT-segmentation\pancreas_segmentation_robust.py"